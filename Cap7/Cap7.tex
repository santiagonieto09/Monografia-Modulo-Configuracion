\mychapter{Capítulo 7. Evaluación de funcionalidades}

En este capítulo, se describen las pruebas realizadas al módulo de configuración, con el objetivo de evaluar sus funcionalidades y verificar su correcto funcionamiento e integración con el resto del sistema.


\section{Pruebas del backend}

La pruebas del backend del módulo de Configuración se llevaron a cabo mediante un enfoque integral que combina diferentes tipos de pruebas. Este proceso de verificación abarca desde la validación de componentes individuales hasta la comprobación de la interacción entre los diferentes elementos del sistema. A continuación, se presentan las pruebas unitarias que validan el comportamiento aislado de cada componente, las pruebas de integración que verifican la correcta comunicación entre servicios y las pruebas funcionales de API REST que confirman el cumplimiento de los requisitos funcionales establecidos.

\subsection{Pruebas unitarias}

Las pruebas unitarias juegan un rol esencial en la ingeniería de software, ya que permiten comprobar el funcionamiento adecuado de partes específicas del código de forma aislada. En el marco de este proyecto, se implementaron pruebas unitarias utilizando JUnit 5 y Mockito, herramientas que colaboran para facilitar la generación de pruebas automáticas. JUnit 5 suministra el esqueleto para planificar, organizar y ejecutar los tests, mientras que Mockito replica el comportamiento de dependencias externas usando objetos simulados, lo que asegura un completo aislamiento de cada unidad durante su evaluación. Esta integración de tecnologías asegura que cada componente funcional satisfaga los criterios establecidos antes de integrarse con el resto del sistema. 

Para obtener pruebas ágiles y eficaces, se optó por evitar contextos pesados como Spring Boot, enfocándose en testar los fragmentos de código de manera independiente. Este método ofrece retroalimentación rápida en el proceso de desarrollo y permite identificar errores en etapas iniciales.

Los aspectos generales de cobertura incluyen:
\begin{itemize}
\item Alta cobertura en las clases de dominio, lógica de negocio, adaptadores de entrada y salida críticos
\item Los Mappers tienen una cobertura de 0\% ya que no se implentan pruebas unitarias para estas clases, dado que se usa @Mapper de MapStruct que genera código automáticamente y no se considera necesario probar estas clases directamente.
\item Otros casos como las configuraciones con @Bean también presentan una cobertura baja debido a la naturaleza declarativa de su implementación.
\end{itemize}

Como se muestra en la Figura \ref{fig:testcatalogue}, se ejecutaron un total de 1496 pruebas unitarias para el microservicio \texttt{account\_catalogue}, todas ellas completadas satisfactoriamente, lo que puede ayudar a una mayor confianza y estabilidad del código implementado.

\begin{figure}[H]
    \centering%
    \includegraphics[width=0.7\textwidth, height=0.7\textheight, keepaspectratio]{Cap7/Figuras/testCatalogue.png}    
    \caption{Resultado ejecución pruebas unitarias microservicio \texttt{account\_catalogue}}
    \label{fig:testcatalogue}
\end{figure}
 
Para asegurar una cobertura adecuada del código, se utilizó JaCoCo (Java Code Coverage), una herramienta que permite medir y visualizar la cobertura de código alcanzada por las pruebas unitarias. Debido a que los paquetes del microservicio \texttt{account\_catalogue} son demasiados se mostraran los resultados por partes, las Figuras \ref{fig:jacocoCataloguep1}, \ref{fig:jacocoCataloguep2} y \ref{fig:jacocoCataloguep3} muestran el informe de cobertura generado por JaCoCo.

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoCataloguep1.png}    
    \caption{Resultados jacoco \texttt{account\_catalogue} parte 1}
    \label{fig:jacocoCataloguep1}
\end{figure}

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoCataloguep2.png}    
    \caption{Resultados jacoco \texttt{account\_catalogue} parte 2}
    \label{fig:jacocoCataloguep2}
\end{figure}

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoCataloguep3.png}    
    \caption{Resultados jacoco \texttt{account\_catalogue} parte 3}
    \label{fig:jacocoCataloguep3}
\end{figure}

El informe de JaCoCo revela una cobertura general del 57\% del código, lo que representa un nivel inferior, esta cobertura reducida se explica principalmente por el hecho de que el microservicio \texttt{account\_catalogue} es compartido entre el módulo de cartera y el módulo contable cartera, lo que implica que las funcionalidades y clases de dichos módulos no se cubrieron en las pruebas unitarias, debido a que están en construcción y no han alcanzado su fase de pruebas.


Como se muestra en la Figura \ref{fig:testconfiguracion}, se ejecutaron un total de 603 pruebas unitarias para el microservicio \texttt{ms-configuration}, todas ellas completadas satisfactoriamente.

\begin{figure}[H]
    \centering%
    \includegraphics[width=0.7\textwidth, height=0.7\textheight, keepaspectratio]{Cap7/Figuras/testConfiguracion.png}    
    \caption{Resultado de ejecución pruebas unitarias \texttt{ms-configuration} }
    \label{fig:testconfiguracion}
\end{figure}

\newpage
Las Figuras \ref{fig:jacocoConfiguracion1} y \ref{fig:jacocoConfiguracion2} muestran el informe de cobertura generado por JaCoCo.


\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoConfig1.png}    
    \caption{Resultados jacoco \texttt{ms-configuration} parte 1}
    \label{fig:jacocoConfiguracion1} 
\end{figure}

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoConfig2.png}    
    \caption{Resultados jacoco \texttt{ms-configuration} parte 2}
    \label{fig:jacocoConfiguracion2} 
\end{figure}

\newpage
El informe de JaCoCo revela una cobertura general del 77\% del código, con algunas clases alcanzando el 100\% de cobertura. Este nivel de cobertura se considera aceptable para el proyecto, aunque se identifican áreas de oportunidad para mejorar
la cobertura en algunas clases específicas.


Como se muestra en la Figura \ref{fig:testproductos}, se ejecutaron un total de 776 pruebas unitarias para el microservicio \texttt{products-management}, todas ellas completadas satisfactoriamente.

\begin{figure}[H]
    \centering%
    \includegraphics[width=0.7\textwidth, height=0.7\textheight, keepaspectratio]{Cap7/Figuras/testProductos.png}    
    \caption{Resultado de ejecución pruebas unitarias \texttt{ms-productos} }
    \label{fig:testproductos} 
\end{figure}

La Figura \ref{fig:jacocoProductos} muestra el informe de cobertura generado por JaCoCo.

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoProductos.png}    
    \caption{Resultado jacoco microservicio \texttt{ms-productos} }
    \label{fig:jacocoProductos} 
\end{figure}

El informe de JaCoCo revela una cobertura general del 81\% del código, con algunas clases alcanzando el 100\% de cobertura. Este nivel de cobertura se considera bastante aceptable para el proyecto, aunque se identifican áreas de oportunidad para mejorar la cobertura en algunas clases específicas.


Como se muestra en la Figura \ref{fig:testterceros}, se ejecutaron un total de 1720 pruebas unitarias para el microservicio \texttt{thirds-management}, todas ellas completadas satisfactoriamente.

\begin{figure}[H]
    \centering%
    \includegraphics[width=0.7\textwidth, height=0.7\textheight, keepaspectratio]{Cap7/Figuras/testTerceros.png}    
    \caption{Resultado de ejecución pruebas unitarias \texttt{ms-terceros} }
    \label{fig:testterceros} 
\end{figure}

La Figura \ref{fig:jacocoTerceros} muestra el informe de cobertura generado por JaCoCo.

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/jacocoTerceros.png}    
    \caption{Resultado jacoco microservicio \texttt{ms-terceros} }
    \label{fig:jacocoTerceros} 
\end{figure}


El informe de JaCoCo revela una cobertura general del 83\% del código, con algunas clases alcanzando el 100\% de cobertura. Este nivel de cobertura se considera bastante aceptable para el proyecto, aunque se identifican áreas de oportunidad para mejorar la cobertura en algunas clases específicas.


\subsection{Pruebas de integración}

Las pruebas de integración representan un elemento esencial para comprobar que los diversos componentes del sistema operen de forma adecuada en conjunto. En el marco de este proyecto, resultaba indispensable asegurar que el módulo de Configuración no solo operara correctamente en aislamiento, sino que además interactuara de manera perfecta y segura con los demás elementos del entorno de microservicios. Una integración apropiada es vital para mantener la coherencia y la integridad de la información a lo largo de toda la aplicación.

\subsection{Estrategia de pruebas de integración}

Para lograr este objetivo se diseñó una estrategia de pruebas de integración de caja negra que simula el comportamiento real del sistema. Dado que el módulo de Configuración opera de forma mayoritariamente síncrona, respondiendo a solicitudes HTTP directas a través de endpoints REST, la estrategia se enfoca en validar el flujo completo de manera directa mediante un proceso de dos etapas:

\begin{enumerate}
\item \textbf{Invocación del endpoint}: Se realiza una llamada HTTP directa al endpoint REST correspondiente. Esta solicitud simula una operación real del sistema, como la creación o consulta de un recurso, permitiendo probar el punto de entrada principal del módulo. Esta etapa valida la capacidad del sistema para procesar y validar correctamente las solicitudes entrantes.

\item \textbf{Verificación del resultado}: Dado que el procesamiento es síncrono, se verifica inmediatamente la respuesta del endpoint y se consulta la base de datos para confirmar el estado actualizado. Esto permite confirmar si la lógica de negocio se ejecutó correctamente y si el estado del sistema se actualizó como se esperaba, validando tanto la persistencia en la base de datos como la correcta exposición de los resultados a través de la API.
\end{enumerate}

Este enfoque valida la integración completa del módulo de Configuración con tres componentes fundamentales de la arquitectura:

\begin{itemize}
\item \textbf{API REST:} Se demuestra que el módulo procesa y responde a las solicitudes correctamente.
\item \textbf{Base de datos:} Se confirma que tras procesar una solicitud, el estado persiste de forma correcta.
\item \textbf{Lógica de validación:} Se asegura que las validaciones se ejecutan de manera correcta.
\end{itemize}

Esta cobertura proporciona una alta confianza en que el módulo no solo realiza sus operaciones de forma correcta, sino que está correctamente integrado en el flujo de trabajo general de la aplicación.


\subsection{Implementación con Postman y Newman}

Se empleó Postman para diseñar las pruebas de integración y Newman para su ejecución automática. El conjunto de pruebas aplica la estrategia previamente detallada, abarcando casos positivos y negativos. La recopilación de pruebas se organizó de manera estructurada, como se muestra en la \autoref{fig:postman}, donde se distinguen dos estructuras principales: la fase de obtención de Token, Init, Integration y Tear Down.

Para los módulos de Tipos de documentos, Calendario contable, Catálogo de cuentas, Centros de costo y Centro de ayuda, no era necesaria la fase Init, puesto que estos módulos no necesitan una inicialización previa de datos para su funcionamiento.

Por otro lado, el módulo de Cuentas bancarias necesita datos iniciales de cuentas contables auxiliares, al igual que Impuestos y Métodos de pago. El módulo de Productos requiere inicialmente un tipo de producto, categoría y unidad de medida. Finalmente, el módulo de Terceros necesita datos como tipos de identificación y tipos de terceros para la correcta ejecución de las pruebas.

\begin{figure}[H]
    \centering%
    \includegraphics[width=0.5\textwidth, height=0.5\textheight, keepaspectratio]{Cap7/Figuras/postman.png}    
    \caption{Estructura de la colección de pruebas en Postman}
    \label{fig:postman} 
\end{figure}

\subsection{Casos de prueba: Escenarios positivos y negativos}

La implementación de pruebas se divide en dos categorías fundamentales que validan la robustez del sistema:

\textbf{Pruebas de caso positivo (Happy Path)}: Estas pruebas validan que, ante una solicitud válida (como crear un tipo de documento con todos sus datos correctos), el sistema procesa la solicitud, actualiza la base de datos correctamente. El flujo de validación consiste en: (1) enviar una solicitud HTTP válida al endpoint correspondiente, (2) verificar la respuesta inmediata, y (3) consultar la base de datos para confirmar que el estado refleja correctamente la operación realizada. Esto asegura que la funcionalidad principal opera según lo diseñado y que la integración entre componentes es exitosa.

\textbf{Pruebas de caso negativo (Sad Path)}: Igualmente importante fue verificar que el sistema maneja los errores de forma robusta y observable. Para ello se envían solicitudes con datos inválidos, como crear un tipo de documento sin nombre o con valores incorrectos. En estos casos, se verifica la respuesta de error. La prueba es exitosa si confirma que el error fue capturado, registrado y expuesto correctamente a través de la API. Esto garantiza que el sistema es resiliente y no falla de manera silenciosa, cumpliendo con el principio de observabilidad necesario en arquitecturas de microservicios.

Para validar los resultados de manera automática, se implementaron scripts post-respuesta en Postman, como se muestra en la Figura \ref{fig:postmanscripts}. Estos scripts permiten ejecutar verificaciones automáticas sobre las respuestas recibidas, validando tanto la estructura de los datos como los valores esperados en cada escenario.

\begin{figure}[H]
    \centering%
    \includegraphics[width=1.0\textwidth, height=1.0\textheight, keepaspectratio]{Cap7/Figuras/postmanscripts.png}    
    \caption{Implementación de scripts post-respuesta en Postman}
    \label{fig:postmanscripts} 
\end{figure}

Los scripts verifican múltiples aspectos de las respuestas:
\begin{itemize}
\item Validación del código de estado HTTP (200 para casos exitosos, códigos de error apropiados para casos negativos)
\item Verificación de la estructura correcta de la respuesta JSON
\item Validación del contenido contra el payload esperado
\item Comprobación de campos obligatorios y sus tipos de datos
\item En casos negativos, validación de la presencia y formato de mensajes de error
\end{itemize}



\subsection{Pruebas funcionales de API REST}

Para estas pruebas de ejecución automatizada se empleó Newman, la interfaz de línea de comandos de Postman. Esta herramienta permite ejecutar colecciones de Postman de manera programada, lo que simplifica su incorporación en pipelines de CI/CD y la creación de informes detallados.

\subsubsection{Entorno de ejecución de pruebas}

Cabe destacar que las pruebas se realizaron en un computador de escritorio con las siguientes características técnicas:

\begin{itemize}
\item Modelo placa base: GIGABYTE H610M K DDR4
\item Procesador: 12th Gen Intel(R) Core(TM) i5-12400F
\item Memoria RAM: 16 GB
\item Tarjeta gráfica:	NVIDIA GeForce GTX 1050 Ti (4 GB)
\item Sistema Operativo: Windows 10 Pro 64 bits
\end{itemize}






Para evaluar la estabilidad y consistencia del sistema bajo múltiples ejecuciones, se llevaron a cabo pruebas iterativas ejecutando la suite completa 20 veces consecutivamente. A continuación, se muestran resultados seleccionados de las pruebas de integración. Para consultar los resultados completos, estos están disponibles en el \autoref{AnexoNewman}.

\subsubsection{Resultados de las pruebas de integración}

 \textbf{Métodos de pago}: Los resultados de las 20 iteraciones en la Figura \ref{fig:metodosdepago} muestran:
    \begin{itemize}
        \item Total de aserciones: 4240 (212 por iteración $\times$ 20 iteraciones), todas exitosas
        \item Pruebas fallidas: 0
        \item Requests ejecutados: 1420
        \item Duración total: 2 minutos 17.1 segundos
        \item Tiempo promedio de respuesta: 12ms
    \end{itemize}

    \begin{figure}[H]
    \centering%
    \includegraphics[width=0.8\textwidth, height=0.8\textheight, keepaspectratio]{Cap7/newman/newmanMetodosPago20.png}    
    \caption{Resultados de iteraciones del submódulo Métodos de Pago}
    \label{fig:metodosdepago} 
    \end{figure}

    \newpage
    \textbf{Centro de ayuda}: Los resultados de las 20 iteraciones en la Figura \ref{fig:centrodeayuda} muestran:
    \begin{itemize}
        \item Total de aserciones: 2580 (129 por iteración $\times$ 20 iteraciones), todas exitosas
        \item Pruebas fallidas: 0
        \item Requests ejecutados: 1220
        \item Duración total: 2 minutos 0.5 segundos
        \item Tiempo promedio de respuesta: 14ms
    \end{itemize}

    \begin{figure}[H]
    \centering%
    \includegraphics[width=0.8\textwidth, height=0.8\textheight, keepaspectratio]{Cap7/newman/newmanCentroAyuda20.png}    
    \caption{Resultados de iteraciones del submódulo Centro de Ayuda}
    \label{fig:centrodeayuda} 
    \end{figure}

\section{Pruebas del frontend}

La validación de la capa de presentación desarrollada en Angular se realizó mediante el framework Jest. Se estableció una estrategia de ejecución unificada que consolida tanto las pruebas unitarias como las de integración en un mismo ciclo de pruebas, diferenciándose del modelo segregado utilizado en el backend.

El alcance de estas pruebas abarca:

\begin{itemize}
\item \textbf{Pruebas unitarias}: Verificación de la lógica interna, cálculos y métodos de los componentes y servicios de Angular, permitiendo que cada unidad funcional opere de forma aislada.

\item \textbf{Pruebas de integración}: Validación del ciclo de vida de los componentes, renderizado del DOM y comunicación entre componentes mediante el uso de dobles de prueba (mocks), que permitió la correcta integración de las diferentes partes de la interfaz de usuario.
\end{itemize}

\section{Pruebas de aceptación}

Las pruebas de aceptación constituyen la etapa de validación final del software, donde se verifica que las funcionalidades implementadas satisfagan efectivamente los requerimientos y expectativas del usuario final quien, en última instancia, determina la calidad y utilidad del sistema desarrollado. En este proyecto, dichas pruebas se ejecutaron de manera virtual mediante sesiones en videoconferencia utilizando la aplicación en la versión de prueba desplegada. 

Durante estas sesiones, los usuarios finales expertos Omar Gomez Gomez y Brayan Neil Vargas, quienes poseen amplia experiencia en el dominio contable y conocimiento profundo de los procesos educativos del programa de Contaduría Pública, evaluaron sistemáticamente las funcionalidades desarrolladas del módulo de Configuración. Los evaluadores interactuaron directamente con la aplicación a través de sus propios dispositivos, navegando por las diferentes interfaces, ejecutando operaciones y explorando los flujos de trabajo implementados. Durante este proceso, no solo verificaban el correcto funcionamiento técnico, sino que también aportaban observaciones valiosas sobre la usabilidad, coherencia pedagógica y pertinencia de las funcionalidades en escenarios reales de uso académico.

La dinámica de las pruebas permitió un intercambio fluido de retroalimentación, donde los evaluadores expresaban sus opiniones, planteaban inquietudes, sugerían mejoras y validaban aspectos específicos del sistema. Cada funcionalidad evaluada recibió un estatus de ``aprobada'' o ``rechazada'' según cumpliera o no con las expectativas y criterios de aceptación previamente establecidos. Esta retroalimentación permitió seguir un ciclo de mejora continua hasta alcanzar la aprobación de cada funcionalidad. El formato estructurado empleado para registrar sistemáticamente estas pruebas de aceptación se presenta en la Tabla \ref{tabla:formato-pruebas-aceptacion}.

\input{Cap7/Tablas/formato.tex}

A continuación se presenta un ejemplo de prueba de aceptación para validar la funcionalidad de creación de tipos de documentos en el módulo de Configuración, como se muestra en la Tabla \ref{tabla:ejemplo-prueba-aceptacion}. Todas las pruebas de aceptación de los submódulos del módulo de Configuración están disponibles en el siguiente \href{https://drive.google.com/drive/folders/1YYcDIIqHlUzBr5Rj65Mry3DT7DZLOnLk?usp=sharing}{\underline{\textcolor{blue}{enlace}}}, donde se organizan de manera sistemática.


\input{Cap7/Tablas/ejemploFormato.tex}